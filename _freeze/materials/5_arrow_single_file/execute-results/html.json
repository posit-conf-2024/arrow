{
  "hash": "8e987b3003a6c0a3bf5337526f5d6598",
  "result": {
    "engine": "knitr",
    "markdown": "---\nfooter: \"[ðŸ”— pos.it/arrow-conf24](https://pos.it/arrow-conf24)\"\nlogo: \"images/logo.png\"\nexecute:\n  echo: true\nformat:\n  revealjs: \n    theme: default\nengine: knitr\neditor: source\n---\n\n\n# Arrow in R: In-Memory Workflows {#single-file-api}\n\n\n::: {.cell}\n\n:::\n\n\n## arrow ðŸ“¦\n\n![](images/arrow-read-write-updated.png)\n\n## Arrow & Single Files\n\n<br>\n\n`library(arrow)`\n\n-   `read_parquet()`\n-   `read_csv_arrow()`\n-   `read_feather()`\n-   `read_json_arrow()`\n\n**Value**: `tibble` (the default), or an Arrow Table if `as_data_frame = FALSE` --- both *in-memory*\n\n## Your Turn\n\n1.  Read in a single NYC Taxi parquet file using `read_parquet()` as an Arrow Table\n\n2.  Convert your Arrow Table object to a `data.frame` or a `tibble`\n\n## Read a Parquet File (`tibble`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arrow)\n\nparquet_file <- \"data/nyc-taxi/year=2019/month=9/part-0.parquet\"\n\ntaxi_df <- read_parquet(file = parquet_file)\ntaxi_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6,567,396 Ã— 22\n   vendor_name pickup_datetime     dropoff_datetime    passenger_count\n   <chr>       <dttm>              <dttm>                        <int>\n 1 CMT         2019-08-31 18:09:30 2019-08-31 18:15:42               1\n 2 CMT         2019-08-31 18:26:30 2019-08-31 18:44:31               1\n 3 CMT         2019-08-31 18:39:35 2019-08-31 19:15:55               2\n 4 VTS         2019-08-31 18:12:26 2019-08-31 18:15:17               4\n 5 VTS         2019-08-31 18:43:16 2019-08-31 18:53:50               1\n 6 VTS         2019-08-31 18:26:13 2019-08-31 18:45:35               1\n 7 CMT         2019-08-31 18:34:52 2019-08-31 18:42:03               1\n 8 CMT         2019-08-31 18:50:02 2019-08-31 18:58:16               1\n 9 CMT         2019-08-31 18:08:02 2019-08-31 18:14:44               0\n10 VTS         2019-08-31 18:11:38 2019-08-31 18:26:47               1\n# â„¹ 6,567,386 more rows\n# â„¹ 18 more variables: trip_distance <dbl>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, rate_code <chr>, store_and_fwd <chr>,\n#   dropoff_longitude <dbl>, dropoff_latitude <dbl>, payment_type <chr>,\n#   fare_amount <dbl>, extra <dbl>, mta_tax <dbl>, tip_amount <dbl>,\n#   tolls_amount <dbl>, total_amount <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, pickup_location_id <int>, â€¦\n```\n\n\n:::\n:::\n\n\n## Read a Parquet File (`Table`)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntaxi_table <- read_parquet(file = parquet_file, as_data_frame = FALSE)\ntaxi_table\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTable\n6567396 rows x 22 columns\n$vendor_name <string>\n$pickup_datetime <timestamp[ms]>\n$dropoff_datetime <timestamp[ms]>\n$passenger_count <int64>\n$trip_distance <double>\n$pickup_longitude <double>\n$pickup_latitude <double>\n$rate_code <string>\n$store_and_fwd <string>\n$dropoff_longitude <double>\n$dropoff_latitude <double>\n$payment_type <string>\n$fare_amount <double>\n$extra <double>\n$mta_tax <double>\n$tip_amount <double>\n$tolls_amount <double>\n$total_amount <double>\n$improvement_surcharge <double>\n$congestion_surcharge <double>\n...\n2 more columns\nUse `schema()` to see entire schema\n```\n\n\n:::\n:::\n\n\n## `tibble` \\<-\\> `Table` \\<-\\> `data.frame`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\n#change a df to a table\narrow_table(taxi_df)\n\n#change a table to a tibble\ntaxi_table |> collect()\nas_tibble(taxi_table)\n\n#change a table to a data.frame\nas.data.frame(taxi_table)\n```\n:::\n\n\n<br>\n\n-   `data.frame` & `tibble` are R objects *in-memory*\n-   `Table` is an Arrow object *in-memory*\n\n## Watch Your Schemas ðŸ‘€\n\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nschema(taxi_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSchema\nvendor_name: string\npickup_datetime: timestamp[us, tz=America/Vancouver]\ndropoff_datetime: timestamp[us, tz=America/Vancouver]\npassenger_count: int32\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int32\ndropoff_location_id: int32\n```\n\n\n:::\n:::\n\n:::\n\n::: {.column width=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nschema(taxi_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSchema\nvendor_name: string\npickup_datetime: timestamp[ms]\ndropoff_datetime: timestamp[ms]\npassenger_count: int64\ntrip_distance: double\npickup_longitude: double\npickup_latitude: double\nrate_code: string\nstore_and_fwd: string\ndropoff_longitude: double\ndropoff_latitude: double\npayment_type: string\nfare_amount: double\nextra: double\nmta_tax: double\ntip_amount: double\ntolls_amount: double\ntotal_amount: double\nimprovement_surcharge: double\ncongestion_surcharge: double\npickup_location_id: int64\ndropoff_location_id: int64\n```\n\n\n:::\n:::\n\n:::\n\n::::\n\n## Data frames\n\n![](images/tabular-structures-r.png)\n\n## Arrow Tables\n\n![](images/tabular-structures-arrow-1.png)\n\n::: notes\nArrow Tables are collections of chunked arrays\n:::\n\n## Table \\| Dataset: A `dplyr` pipeline\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 4\n  vendor_name all_trips shared_trips pct_shared\n  <chr>           <int>        <int>      <dbl>\n1 VTS           4238808      1339478       31.6\n2 CMT           2294473       470344       20.5\n3 <NA>            34115            0        0  \n```\n\n\n:::\n:::\n\n\n<br>\n\nFunctions available in Arrow dplyr queries: <https://arrow.apache.org/docs/r/reference/acero.html>\n\n::: notes\nAll the same capabilities as you practiced with Arrow Dataset\n:::\n\n## Arrow for Efficient In-Memory Processing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet() |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6567396\n```\n\n\n:::\n:::\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2,8\"}\nparquet_file |>\n  read_parquet() |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  1.157   0.261   0.509 \n```\n\n\n:::\n:::\n\n\n## Arrow for Efficient In-Memory Processing\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  nrow()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6567396\n```\n\n\n:::\n:::\n\n\n<br>\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2,8\"}\nparquet_file |>\n  read_parquet(as_data_frame = FALSE) |>\n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  1.047   0.203   0.220 \n```\n\n\n:::\n:::\n\n\n## Read a Parquet File Selectively\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparquet_file |>\n  read_parquet(\n    col_select = c(\"vendor_name\", \"passenger_count\"),\n    as_data_frame = FALSE\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTable\n6567396 rows x 2 columns\n$vendor_name <string>\n$passenger_count <int64>\n```\n\n\n:::\n:::\n\n\n## Selective Reads Are Faster\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|2,3,4,11\"}\nparquet_file |>\n  read_parquet(\n    col_select = c(\"vendor_name\", \"passenger_count\"),\n    as_data_frame = FALSE\n  ) |> \n  group_by(vendor_name) |>\n  summarise(all_trips = n(),\n            shared_trips = sum(passenger_count > 1, na.rm = TRUE)) |>\n  mutate(pct_shared = shared_trips / all_trips * 100) |>\n  collect() |>\n  system.time()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   user  system elapsed \n  0.258   0.011   0.131 \n```\n\n\n:::\n:::\n\n\n\n:::notes\nNotes: row-based format readers often allow you to specify which columns to read in but the entire row must be read in and the unwanted columns discarded. Parquetâ€™s columnar format allows you to read in only the columns you need, which is faster when you only need a subset of the data.\n:::\n\n## Arrow Table or Dataset?\n\n![](images/2022-09-decision-map.png){.absolute left=\"200\" height=\"550\"}\n\n::: {style=\"font-size: 60%; margin-top: 575px; margin-left: 250px;\"}\n<https://francoismichonneau.net/2022/10/import-big-csv/>\n:::\n\n## Arrow for Improving Those Sluggish Worklows\n\n-   a \"drop-in\" for many dplyr workflows (Arrow Table or Dataset)\n-   works when your tabular data get too big for your RAM (Arrow Dataset)\n-   provides tools for re-engineering data storage for better performance (`arrow::write_dataset()`)\n\n::: notes\nLot's of ways to speed up sluggish workflows e.g. [writing more performant tidyverse code](https://www.tidyverse.org/blog/2023/04/performant-packages/), use other data frame libraries like data.table or polars, use duckDB or other databases, Spark + splarklyr ... However, Arrow offers some attractive features for tackling this challenge, especially for dplyr users.\n:::\n",
    "supporting": [
      "5_arrow_single_file_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}